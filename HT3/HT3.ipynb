{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Set up environment variables and create a spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-1.8.0-openjdk-amd64/'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = '/home/mdldml/spark-2.2.0-bin-hadoop2.7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/lib/py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf().setMaster(\"local[4]\")\n",
    "        .setAppName(\"HT3\")\n",
    "        .set(\"spark.executor.memory\", \"1g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (0.5) implement TF-IDF feature extractor, test it on Amazon reviews dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    result = doc.lower()\n",
    "    for ch in '.,?!:;(){}[]-\"''':\n",
    "        result = result.replace(ch, ' ')\n",
    "    return result.split()[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function which computes TF-IDF using Spark. Takes RDD[unicode], returns RDD[(documentId, tokenId, score)] and two mappings (correspondence between docs and their ids, similar for tokens, both also RDD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_tf_idf(docs):\n",
    "    totalDocs = docs.count()\n",
    "    docIds = docs.zipWithIndex()\n",
    "    \n",
    "    tokens = docs.flatMap(tokenize).distinct()\n",
    "    tokenIds = tokens.zipWithIndex()\n",
    "\n",
    "    tokensWithDocIds = docIds.flatMap(lambda (doc, docId): [(token, docId) for token in tokenize(doc)])\n",
    "    idPairs = tokensWithDocIds.join(tokenIds).map(lambda (token, ids): (ids, 1))\n",
    "    occurences = idPairs.reduceByKey(add)\n",
    "\n",
    "    tokensByDoc = occurences.map(lambda ((docId, tokenId), occs): (docId, occs)).reduceByKey(add)\n",
    "    tf = occurences.map(lambda ((docId, tokenId), occs): (docId, (tokenId, occs))) \\\n",
    "                    .join(tokensByDoc) \\\n",
    "                    .map(lambda (docId, ((tokenId, occs), totalOccs)): ((docId, tokenId), 1.0 * occs / totalOccs))\n",
    "\n",
    "    docsWithToken = occurences.map(lambda ((docId, tokenId), occs): (tokenId, 1)).reduceByKey(add)\n",
    "    idf = occurences.map(lambda ((docId, tokenId), occs): (tokenId, (docId, occs))) \\\n",
    "                    .join(docsWithToken) \\\n",
    "                    .map(lambda (tokenId, ((docId, occs), totalOccs)): ((docId, tokenId), log(totalDocs / totalOccs))) \\\n",
    "                    .filter(lambda ((docId, tokenId), idf): idf > 0.0)\n",
    "\n",
    "    tf_idf = tf.join(idf).map(lambda ((docId, tokenId), (tf, idf)): ((docId, tokenId), tf * idf))\n",
    "    \n",
    "    return tf_idf, docIds, tokenIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute top-20 TF-IDF sums for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def top_tf_idf_by_class(docs):\n",
    "    flip = lambda (k, v) : (v, k)\n",
    "    \n",
    "    for docClass in ['__label__1', '__label__2']:\n",
    "        docSet = docs.filter(lambda doc: doc.startswith(docClass))\n",
    "        tf_idf, docIds, tokenIds = compute_tf_idf(docSet)\n",
    "        scoreSums = tf_idf.map(lambda ((docId, tokenId), score): (tokenId, score)).reduceByKey(add)\n",
    "        tokenSums = tokenIds.map(flip).join(scoreSums).map(lambda (tokenId, (token, score)): (token, score))\n",
    "        topScores = tokenSums.top(20, key=lambda (token, score): score)\n",
    "        \n",
    "        print(docClass + ': ')\n",
    "        print('\\n'.join(['%s: %f' % (token, score) for (token, score) in topScores]))\n",
    "        print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1: \n",
      "book: 16330.895751\n",
      "was: 12885.357014\n",
      "my: 11994.103282\n",
      "that: 11469.250495\n",
      "movie: 10358.086593\n",
      "but: 10236.770890\n",
      "are: 10091.592524\n",
      "very: 9900.274922\n",
      "as: 9841.842132\n",
      "all: 9669.699670\n",
      "be: 9586.675211\n",
      "no: 9558.854168\n",
      "they: 9450.397920\n",
      "one: 9433.204380\n",
      "like: 9342.514748\n",
      "good: 9305.327600\n",
      "just: 9234.535986\n",
      "on: 9077.007486\n",
      "would: 8903.926978\n",
      "you: 8892.498017\n",
      "----------\n",
      "__label__2: \n",
      "great: 16505.712062\n",
      "book: 15774.071349\n",
      "was: 14761.892442\n",
      "good: 14002.563714\n",
      "very: 12586.753536\n",
      "as: 11526.680868\n",
      "movie: 11079.791894\n",
      "are: 10867.738195\n",
      "love: 10538.755586\n",
      "all: 10482.726956\n",
      "you: 10323.048546\n",
      "that: 10303.384281\n",
      "read: 10234.146347\n",
      "they: 9955.414737\n",
      "so: 9858.152240\n",
      "my: 9847.553804\n",
      "not: 9761.534446\n",
      "one: 9694.977640\n",
      "be: 9575.281311\n",
      "with: 9386.017724\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "docs = sc.textFile(\"train.ft.txt\")\n",
    "top_tf_idf_by_class(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (0.5) implement and train logistic regression on MNIST:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return (data / np.float32(256)).squeeze()\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    #X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    #y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape as [batch_size x total_features]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(len(X_train), -1), X_test.reshape(len(X_test), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    logits = np.exp(x - np.max(x))  # for numeric stability\n",
    "    return logits / np.sum(logits, axis=0) if logits.ndim == 1 else logits / np.array([np.sum(logits, axis=1)]).T  # 2d version was used for draft non-spark version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression train function. Unlike in previous exercise, it takes regular numpy dataset and encapsulates work with Spark: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_train(X, y, learning_rate=1.0, epochs=100):\n",
    "    n, d = X.shape\n",
    "    m = 1 + np.max(y)\n",
    "    \n",
    "    y_wide = np.zeros((n, m))  # 1/0 answer for every class for convinience\n",
    "    y_wide[np.arange(n), y] = 1.0\n",
    "    \n",
    "    W = np.random.random((d, m)) - 0.5\n",
    "    b = np.zeros(m)\n",
    "    \n",
    "    data = sc.parallelize(zip(list(X), list(y), list(y_wide)))  # work with Spark begins here\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        logits = data.map(lambda (X_i, y_i, y_wide_i): softmax(X_i.dot(W) + b))\n",
    "        preds = logits.map(lambda logits_i: np.argmax(logits_i))\n",
    "        \n",
    "        dataWithLogits = data.zip(logits)\n",
    "        loss = dataWithLogits.map(lambda ((X_i, y_i, y_wide_i), logits_i): -np.log(logits_i[y_i])).mean()\n",
    "        accuracy = 100.0 * preds.zip(data).map(lambda (preds_i, (X_i, y_i, y_wide_i)): 1.0 if preds_i == y_i else 0.0).mean()\n",
    "        \n",
    "        W -= learning_rate * dataWithLogits.map(lambda ((X_i, y_i, y_wide_i), logits_i): np.outer(X_i, logits_i - y_wide_i)).mean()\n",
    "        b -= learning_rate * dataWithLogits.map(lambda ((X_i, y_i, y_wide_i), logits_i): logits_i - y_wide_i).mean()\n",
    "        \n",
    "        print('Epoch %d:' % i)\n",
    "        print('\\tloss:\\t\\t%f\\n\\taccuracy:\\t%.4f%%' % (loss, accuracy))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "\tloss:\t\t4.341922\n",
      "\taccuracy:\t8.6783%\n",
      "Epoch 1:\n",
      "\tloss:\t\t3.582653\n",
      "\taccuracy:\t22.2800%\n",
      "Epoch 2:\n",
      "\tloss:\t\t2.934465\n",
      "\taccuracy:\t24.4550%\n",
      "Epoch 3:\n",
      "\tloss:\t\t2.511403\n",
      "\taccuracy:\t39.2667%\n",
      "Epoch 4:\n",
      "\tloss:\t\t2.014235\n",
      "\taccuracy:\t47.3533%\n",
      "Epoch 5:\n",
      "\tloss:\t\t1.794536\n",
      "\taccuracy:\t51.4800%\n",
      "Epoch 6:\n",
      "\tloss:\t\t1.370442\n",
      "\taccuracy:\t60.9750%\n",
      "Epoch 7:\n",
      "\tloss:\t\t1.059472\n",
      "\taccuracy:\t66.2833%\n",
      "Epoch 8:\n",
      "\tloss:\t\t1.003032\n",
      "\taccuracy:\t67.7467%\n",
      "Epoch 9:\n",
      "\tloss:\t\t0.966827\n",
      "\taccuracy:\t69.0867%\n",
      "Epoch 10:\n",
      "\tloss:\t\t0.998471\n",
      "\taccuracy:\t69.0750%\n",
      "Epoch 11:\n",
      "\tloss:\t\t0.913067\n",
      "\taccuracy:\t71.0683%\n",
      "Epoch 12:\n",
      "\tloss:\t\t0.901040\n",
      "\taccuracy:\t70.8233%\n",
      "Epoch 13:\n",
      "\tloss:\t\t0.874549\n",
      "\taccuracy:\t72.1900%\n",
      "Epoch 14:\n",
      "\tloss:\t\t0.865013\n",
      "\taccuracy:\t72.1583%\n",
      "Epoch 15:\n",
      "\tloss:\t\t0.806786\n",
      "\taccuracy:\t74.3150%\n",
      "Epoch 16:\n",
      "\tloss:\t\t0.788362\n",
      "\taccuracy:\t74.5850%\n",
      "Epoch 17:\n",
      "\tloss:\t\t0.735690\n",
      "\taccuracy:\t76.6150%\n",
      "Epoch 18:\n",
      "\tloss:\t\t0.710541\n",
      "\taccuracy:\t77.2067%\n",
      "Epoch 19:\n",
      "\tloss:\t\t0.674186\n",
      "\taccuracy:\t78.7233%\n",
      "Epoch 20:\n",
      "\tloss:\t\t0.650285\n",
      "\taccuracy:\t79.4500%\n",
      "Epoch 21:\n",
      "\tloss:\t\t0.626406\n",
      "\taccuracy:\t80.4767%\n",
      "Epoch 22:\n",
      "\tloss:\t\t0.607821\n",
      "\taccuracy:\t81.0000%\n",
      "Epoch 23:\n",
      "\tloss:\t\t0.592085\n",
      "\taccuracy:\t81.7467%\n",
      "Epoch 24:\n",
      "\tloss:\t\t0.578918\n",
      "\taccuracy:\t82.1200%\n",
      "Epoch 25:\n",
      "\tloss:\t\t0.568126\n",
      "\taccuracy:\t82.5817%\n",
      "Epoch 26:\n",
      "\tloss:\t\t0.558796\n",
      "\taccuracy:\t82.8267%\n",
      "Epoch 27:\n",
      "\tloss:\t\t0.550882\n",
      "\taccuracy:\t83.1250%\n",
      "Epoch 28:\n",
      "\tloss:\t\t0.543854\n",
      "\taccuracy:\t83.3533%\n",
      "Epoch 29:\n",
      "\tloss:\t\t0.537601\n",
      "\taccuracy:\t83.5783%\n",
      "Epoch 30:\n",
      "\tloss:\t\t0.531896\n",
      "\taccuracy:\t83.8000%\n",
      "Epoch 31:\n",
      "\tloss:\t\t0.526639\n",
      "\taccuracy:\t83.9983%\n",
      "Epoch 32:\n",
      "\tloss:\t\t0.521739\n",
      "\taccuracy:\t84.1633%\n",
      "Epoch 33:\n",
      "\tloss:\t\t0.517133\n",
      "\taccuracy:\t84.3350%\n",
      "Epoch 34:\n",
      "\tloss:\t\t0.512780\n",
      "\taccuracy:\t84.4633%\n",
      "Epoch 35:\n",
      "\tloss:\t\t0.508647\n",
      "\taccuracy:\t84.6583%\n",
      "Epoch 36:\n",
      "\tloss:\t\t0.504710\n",
      "\taccuracy:\t84.7583%\n",
      "Epoch 37:\n",
      "\tloss:\t\t0.500951\n",
      "\taccuracy:\t84.8867%\n",
      "Epoch 38:\n",
      "\tloss:\t\t0.497353\n",
      "\taccuracy:\t85.0083%\n",
      "Epoch 39:\n",
      "\tloss:\t\t0.493905\n",
      "\taccuracy:\t85.1383%\n",
      "Epoch 40:\n",
      "\tloss:\t\t0.490594\n",
      "\taccuracy:\t85.2533%\n",
      "Epoch 41:\n",
      "\tloss:\t\t0.487412\n",
      "\taccuracy:\t85.3617%\n",
      "Epoch 42:\n",
      "\tloss:\t\t0.484350\n",
      "\taccuracy:\t85.4533%\n",
      "Epoch 43:\n",
      "\tloss:\t\t0.481400\n",
      "\taccuracy:\t85.5900%\n",
      "Epoch 44:\n",
      "\tloss:\t\t0.478556\n",
      "\taccuracy:\t85.6717%\n",
      "Epoch 45:\n",
      "\tloss:\t\t0.475810\n",
      "\taccuracy:\t85.7750%\n",
      "Epoch 46:\n",
      "\tloss:\t\t0.473157\n",
      "\taccuracy:\t85.8483%\n",
      "Epoch 47:\n",
      "\tloss:\t\t0.470593\n",
      "\taccuracy:\t85.9250%\n",
      "Epoch 48:\n",
      "\tloss:\t\t0.468112\n",
      "\taccuracy:\t86.0000%\n",
      "Epoch 49:\n",
      "\tloss:\t\t0.465709\n",
      "\taccuracy:\t86.0783%\n",
      "Epoch 50:\n",
      "\tloss:\t\t0.463381\n",
      "\taccuracy:\t86.1717%\n",
      "Epoch 51:\n",
      "\tloss:\t\t0.461124\n",
      "\taccuracy:\t86.2367%\n",
      "Epoch 52:\n",
      "\tloss:\t\t0.458933\n",
      "\taccuracy:\t86.3167%\n",
      "Epoch 53:\n",
      "\tloss:\t\t0.456807\n",
      "\taccuracy:\t86.4100%\n",
      "Epoch 54:\n",
      "\tloss:\t\t0.454742\n",
      "\taccuracy:\t86.4667%\n",
      "Epoch 55:\n",
      "\tloss:\t\t0.452734\n",
      "\taccuracy:\t86.5533%\n",
      "Epoch 56:\n",
      "\tloss:\t\t0.450782\n",
      "\taccuracy:\t86.6267%\n",
      "Epoch 57:\n",
      "\tloss:\t\t0.448882\n",
      "\taccuracy:\t86.6733%\n",
      "Epoch 58:\n",
      "\tloss:\t\t0.447033\n",
      "\taccuracy:\t86.7250%\n",
      "Epoch 59:\n",
      "\tloss:\t\t0.445232\n",
      "\taccuracy:\t86.7867%\n",
      "Epoch 60:\n",
      "\tloss:\t\t0.443477\n",
      "\taccuracy:\t86.8467%\n",
      "Epoch 61:\n",
      "\tloss:\t\t0.441765\n",
      "\taccuracy:\t86.8967%\n",
      "Epoch 62:\n",
      "\tloss:\t\t0.440097\n",
      "\taccuracy:\t86.9617%\n",
      "Epoch 63:\n",
      "\tloss:\t\t0.438468\n",
      "\taccuracy:\t87.0283%\n",
      "Epoch 64:\n",
      "\tloss:\t\t0.436879\n",
      "\taccuracy:\t87.0867%\n",
      "Epoch 65:\n",
      "\tloss:\t\t0.435327\n",
      "\taccuracy:\t87.1467%\n",
      "Epoch 66:\n",
      "\tloss:\t\t0.433810\n",
      "\taccuracy:\t87.1983%\n",
      "Epoch 67:\n",
      "\tloss:\t\t0.432329\n",
      "\taccuracy:\t87.2467%\n",
      "Epoch 68:\n",
      "\tloss:\t\t0.430880\n",
      "\taccuracy:\t87.2850%\n",
      "Epoch 69:\n",
      "\tloss:\t\t0.429463\n",
      "\taccuracy:\t87.3483%\n",
      "Epoch 70:\n",
      "\tloss:\t\t0.428077\n",
      "\taccuracy:\t87.3967%\n",
      "Epoch 71:\n",
      "\tloss:\t\t0.426721\n",
      "\taccuracy:\t87.4467%\n",
      "Epoch 72:\n",
      "\tloss:\t\t0.425393\n",
      "\taccuracy:\t87.4883%\n",
      "Epoch 73:\n",
      "\tloss:\t\t0.424093\n",
      "\taccuracy:\t87.5367%\n",
      "Epoch 74:\n",
      "\tloss:\t\t0.422819\n",
      "\taccuracy:\t87.5650%\n",
      "Epoch 75:\n",
      "\tloss:\t\t0.421571\n",
      "\taccuracy:\t87.6133%\n",
      "Epoch 76:\n",
      "\tloss:\t\t0.420348\n",
      "\taccuracy:\t87.6517%\n",
      "Epoch 77:\n",
      "\tloss:\t\t0.419148\n",
      "\taccuracy:\t87.7067%\n",
      "Epoch 78:\n",
      "\tloss:\t\t0.417972\n",
      "\taccuracy:\t87.7483%\n",
      "Epoch 79:\n",
      "\tloss:\t\t0.416819\n",
      "\taccuracy:\t87.7900%\n",
      "Epoch 80:\n",
      "\tloss:\t\t0.415687\n",
      "\taccuracy:\t87.8183%\n",
      "Epoch 81:\n",
      "\tloss:\t\t0.414576\n",
      "\taccuracy:\t87.8733%\n",
      "Epoch 82:\n",
      "\tloss:\t\t0.413485\n",
      "\taccuracy:\t87.9100%\n",
      "Epoch 83:\n",
      "\tloss:\t\t0.412415\n",
      "\taccuracy:\t87.9417%\n",
      "Epoch 84:\n",
      "\tloss:\t\t0.411363\n",
      "\taccuracy:\t87.9783%\n",
      "Epoch 85:\n",
      "\tloss:\t\t0.410330\n",
      "\taccuracy:\t88.0050%\n",
      "Epoch 86:\n",
      "\tloss:\t\t0.409315\n",
      "\taccuracy:\t88.0367%\n",
      "Epoch 87:\n",
      "\tloss:\t\t0.408317\n",
      "\taccuracy:\t88.0617%\n",
      "Epoch 88:\n",
      "\tloss:\t\t0.407336\n",
      "\taccuracy:\t88.0750%\n",
      "Epoch 89:\n",
      "\tloss:\t\t0.406372\n",
      "\taccuracy:\t88.1033%\n",
      "Epoch 90:\n",
      "\tloss:\t\t0.405424\n",
      "\taccuracy:\t88.1283%\n",
      "Epoch 91:\n",
      "\tloss:\t\t0.404491\n",
      "\taccuracy:\t88.1717%\n",
      "Epoch 92:\n",
      "\tloss:\t\t0.403574\n",
      "\taccuracy:\t88.1900%\n",
      "Epoch 93:\n",
      "\tloss:\t\t0.402671\n",
      "\taccuracy:\t88.2183%\n",
      "Epoch 94:\n",
      "\tloss:\t\t0.401783\n",
      "\taccuracy:\t88.2417%\n",
      "Epoch 95:\n",
      "\tloss:\t\t0.400908\n",
      "\taccuracy:\t88.2800%\n",
      "Epoch 96:\n",
      "\tloss:\t\t0.400047\n",
      "\taccuracy:\t88.3033%\n",
      "Epoch 97:\n",
      "\tloss:\t\t0.399199\n",
      "\taccuracy:\t88.3400%\n",
      "Epoch 98:\n",
      "\tloss:\t\t0.398364\n",
      "\taccuracy:\t88.3700%\n",
      "Epoch 99:\n",
      "\tloss:\t\t0.397542\n",
      "\taccuracy:\t88.3967%\n"
     ]
    }
   ],
   "source": [
    "W, b = logistic_regression_train(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check test dataset accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_predict(X, W, b):\n",
    "    data = sc.parallelize(X)\n",
    "    logits = data.map(lambda X_i: softmax(X_i.dot(W) + b))\n",
    "    return logits.map(lambda logits_i: np.argmax(logits_i)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_check(X, y, W, b):\n",
    "    preds = sc.parallelize(logistic_regression_predict(X, W, b))\n",
    "    y_p = sc.parallelize(y)\n",
    "    \n",
    "    accuracy = 100.0 * preds.zip(y_p).map(lambda (preds_i, y_i): 1.0 if preds_i == y_i else 0.0).mean()\n",
    "\n",
    "    print('accuracy:\\t%.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:\t88.9700%\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_check(X_test, y_test, W, b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
