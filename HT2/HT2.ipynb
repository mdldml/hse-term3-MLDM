{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "def load_dataset():\n",
    "    # We first define a download function, supporting both Python 2 and 3.\n",
    "    if sys.version_info[0] == 2:\n",
    "        from urllib import urlretrieve\n",
    "    else:\n",
    "        from urllib.request import urlretrieve\n",
    "\n",
    "    def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "        print(\"Downloading %s\" % filename)\n",
    "        urlretrieve(source + filename, filename)\n",
    "\n",
    "    # We then define functions for loading MNIST images and labels.\n",
    "    # For convenience, they also download the requested files if needed.\n",
    "    import gzip\n",
    "\n",
    "    def load_mnist_images(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the inputs in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "        # following the shape convention: (examples, channels, rows, columns)\n",
    "        data = data.reshape(-1, 1, 28, 28)\n",
    "        # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "        # (Actually to range [0, 255/256], for compatibility to the version\n",
    "        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "        return (data / np.float32(256)).squeeze()\n",
    "\n",
    "    def load_mnist_labels(filename):\n",
    "        if not os.path.exists(filename):\n",
    "            download(filename)\n",
    "        # Read the labels in Yann LeCun's binary format.\n",
    "        with gzip.open(filename, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        # The labels are vectors of integers now, that's exactly what we want.\n",
    "        return data\n",
    "\n",
    "    # We can now download and read the training and test set images and labels.\n",
    "    X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "    y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "    X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "    y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # We reserve the last 10000 training examples for validation.\n",
    "    #X_train, X_val = X_train[:-10000], X_train[-10000:]\n",
    "    #y_train, y_val = y_train[:-10000], y_train[-10000:]\n",
    "\n",
    "    # We just return all the arrays in order, as expected in main().\n",
    "    # (It doesn't matter how we do this as long as we can read them again.)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz\n",
      "Downloading train-labels-idx1-ubyte.gz\n",
      "Downloading t10k-images-idx3-ubyte.gz\n",
      "Downloading t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convinience, reshape dataset as images consisting of one channel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_train, h, w = X_train.shape\n",
    "len_test = len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((len_train, 1, h, w))\n",
    "X_test = X_test.reshape((len_test, 1, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train the baseline network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_X = T.tensor4(\"X\")\n",
    "\n",
    "input_shape = [None, 1, h, w]\n",
    "target_y = T.vector(\"target Y integer\",dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "conv1_1     = L.layers.Conv2DLayer(input_layer, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "conv1_2     = L.layers.Conv2DLayer(conv1_1, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "pool1       = L.layers.MaxPool2DLayer(conv1_2, pool_size=(2, 2))\n",
    "conv2_1     = L.layers.Conv2DLayer(pool1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "conv2_2     = L.layers.Conv2DLayer(conv2_1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "pool2       = L.layers.MaxPool2DLayer(conv2_2, pool_size=(2, 2))\n",
    "conv3       = L.layers.Conv2DLayer(pool2, num_filters=16, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "pool3       = L.layers.MaxPool2DLayer(conv3, pool_size=(2, 2))\n",
    "dense       = L.layers.DenseLayer(pool3, num_units=10, nonlinearity=L.nonlinearities.softmax)\n",
    "\n",
    "predicted_y = L.layers.get_output(dense)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(predicted_y, target_y).mean()\n",
    "accuracy = L.objectives.categorical_accuracy(predicted_y, target_y).mean()\n",
    "updates = L.updates.adamax(loss, L.layers.get_all_params(dense, trainable=True))\n",
    "\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for iterating batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(X, y, batch_size):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_index in range(0, len(X) - batch_size + 1, batch_size):\n",
    "        excerpt = indices[start_index:(start_index + batch_size)]\n",
    "\n",
    "        yield X[excerpt], y[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for resetting all network weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_weights():\n",
    "    params = L.layers.get_all_params(dense, trainable=True)\n",
    "    \n",
    "    for v in params:\n",
    "        val = v.get_value()\n",
    "        if(len(val.shape) < 2):\n",
    "            v.set_value(L.init.Constant(0.0)(val.shape))\n",
    "        else:\n",
    "            v.set_value(L.init.GlorotUniform()(val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_train(num_epochs=10, batch_size=500):\n",
    "    reset_weights()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = 0\n",
    "        train_acc = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "            inputs, targets = batch\n",
    "            train_err_batch, train_acc_batch = train_fun(inputs, targets)\n",
    "            train_err += train_err_batch\n",
    "            train_acc += train_acc_batch\n",
    "            train_batches += 1\n",
    "\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_test, y_test, batch_size):\n",
    "            inputs, targets = batch\n",
    "            val_acc += accuracy_fun(inputs, targets)\n",
    "            val_batches += 1\n",
    "\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "        print(\"  training loss (in-iteration):\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  train accuracy:\\t\\t{:.2f} %\".format(\n",
    "            train_acc / train_batches * 100))\n",
    "        print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "            val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 58.183s\n",
      "  training loss (in-iteration):\t\t2.196295\n",
      "  train accuracy:\t\t27.74 %\n",
      "  validation accuracy:\t\t45.31 %\n",
      "Epoch 2 of 10 took 57.583s\n",
      "  training loss (in-iteration):\t\t0.989165\n",
      "  train accuracy:\t\t69.23 %\n",
      "  validation accuracy:\t\t84.00 %\n",
      "Epoch 3 of 10 took 57.324s\n",
      "  training loss (in-iteration):\t\t0.444850\n",
      "  train accuracy:\t\t86.65 %\n",
      "  validation accuracy:\t\t90.19 %\n",
      "Epoch 4 of 10 took 57.094s\n",
      "  training loss (in-iteration):\t\t0.312953\n",
      "  train accuracy:\t\t90.69 %\n",
      "  validation accuracy:\t\t91.89 %\n",
      "Epoch 5 of 10 took 57.700s\n",
      "  training loss (in-iteration):\t\t0.254560\n",
      "  train accuracy:\t\t92.42 %\n",
      "  validation accuracy:\t\t93.07 %\n",
      "Epoch 6 of 10 took 56.133s\n",
      "  training loss (in-iteration):\t\t0.218833\n",
      "  train accuracy:\t\t93.45 %\n",
      "  validation accuracy:\t\t93.92 %\n",
      "Epoch 7 of 10 took 57.269s\n",
      "  training loss (in-iteration):\t\t0.190671\n",
      "  train accuracy:\t\t94.22 %\n",
      "  validation accuracy:\t\t94.39 %\n",
      "Epoch 8 of 10 took 57.476s\n",
      "  training loss (in-iteration):\t\t0.171113\n",
      "  train accuracy:\t\t94.84 %\n",
      "  validation accuracy:\t\t95.17 %\n",
      "Epoch 9 of 10 took 57.216s\n",
      "  training loss (in-iteration):\t\t0.159280\n",
      "  train accuracy:\t\t95.17 %\n",
      "  validation accuracy:\t\t95.04 %\n",
      "Epoch 10 of 10 took 57.485s\n",
      "  training loss (in-iteration):\t\t0.144402\n",
      "  train accuracy:\t\t95.55 %\n",
      "  validation accuracy:\t\t95.61 %\n"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline model accuracy after 10 epochs is about 95.6%. It can be trained further up to 98%, after 10-20 more epochs. But for brief comparison of different architectures, this should be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.125) change non-linearity of convolutional layers to sigmoid;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "conv1_1     = L.layers.Conv2DLayer(input_layer, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.sigmoid)\n",
    "conv1_2     = L.layers.Conv2DLayer(conv1_1, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.sigmoid)\n",
    "pool1       = L.layers.MaxPool2DLayer(conv1_2, pool_size=(2, 2))\n",
    "conv2_1     = L.layers.Conv2DLayer(pool1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.sigmoid)\n",
    "conv2_2     = L.layers.Conv2DLayer(conv2_1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.sigmoid)\n",
    "pool2       = L.layers.MaxPool2DLayer(conv2_2, pool_size=(2, 2))\n",
    "conv3       = L.layers.Conv2DLayer(pool2, num_filters=16, filter_size=(3, 3), nonlinearity=L.nonlinearities.sigmoid)\n",
    "pool3       = L.layers.MaxPool2DLayer(conv3, pool_size=(2, 2))\n",
    "dense       = L.layers.DenseLayer(pool3, num_units=10, nonlinearity=L.nonlinearities.sigmoid)\n",
    "\n",
    "predicted_y = L.layers.get_output(dense)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(predicted_y, target_y).mean()\n",
    "accuracy = L.objectives.categorical_accuracy(predicted_y, target_y).mean()\n",
    "updates = L.updates.adamax(loss, L.layers.get_all_params(dense, trainable=True))\n",
    "\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 76.429s\n",
      "  training loss (in-iteration):\t\t0.218000\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 2 of 10 took 75.975s\n",
      "  training loss (in-iteration):\t\t0.068098\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 3 of 10 took 77.436s\n",
      "  training loss (in-iteration):\t\t0.038532\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 4 of 10 took 76.923s\n",
      "  training loss (in-iteration):\t\t0.025923\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 5 of 10 took 76.091s\n",
      "  training loss (in-iteration):\t\t0.018941\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 6 of 10 took 77.550s\n",
      "  training loss (in-iteration):\t\t0.014529\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 7 of 10 took 77.134s\n",
      "  training loss (in-iteration):\t\t0.011505\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 8 of 10 took 76.313s\n",
      "  training loss (in-iteration):\t\t0.009318\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 9 of 10 took 76.413s\n",
      "  training loss (in-iteration):\t\t0.007673\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n",
      "Epoch 10 of 10 took 77.337s\n",
      "  training loss (in-iteration):\t\t0.006399\n",
      "  train accuracy:\t\t10.44 %\n",
      "  validation accuracy:\t\t10.28 %\n"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that LeakyReLU is better than sigmoid! With latter, are stuck in some local optima and can't go further than 10% accuracy. Probably that can be fixed by changing learning rate or other training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.125) change non-linearity of convolutional layers to ELU;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "conv1_1     = L.layers.Conv2DLayer(input_layer, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.elu)\n",
    "conv1_2     = L.layers.Conv2DLayer(conv1_1, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.elu)\n",
    "pool1       = L.layers.MaxPool2DLayer(conv1_2, pool_size=(2, 2))\n",
    "conv2_1     = L.layers.Conv2DLayer(pool1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.elu)\n",
    "conv2_2     = L.layers.Conv2DLayer(conv2_1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.elu)\n",
    "pool2       = L.layers.MaxPool2DLayer(conv2_2, pool_size=(2, 2))\n",
    "conv3       = L.layers.Conv2DLayer(pool2, num_filters=16, filter_size=(3, 3), nonlinearity=L.nonlinearities.elu)\n",
    "pool3       = L.layers.MaxPool2DLayer(conv3, pool_size=(2, 2))\n",
    "dense       = L.layers.DenseLayer(pool3, num_units=10, nonlinearity=L.nonlinearities.elu)\n",
    "\n",
    "predicted_y = L.layers.get_output(dense)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(predicted_y, target_y).mean()\n",
    "accuracy = L.objectives.categorical_accuracy(predicted_y, target_y).mean()\n",
    "updates = L.updates.adamax(loss, L.layers.get_all_params(dense, trainable=True))\n",
    "\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 87.807s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.74 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 2 of 10 took 87.801s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 3 of 10 took 87.989s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 4 of 10 took 89.569s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 5 of 10 took 92.911s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 6 of 10 took 100.801s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 7 of 10 took 90.738s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 8 of 10 took 92.707s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 9 of 10 took 97.285s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n",
      "Epoch 10 of 10 took 94.427s\n",
      "  training loss (in-iteration):\t\tnan\n",
      "  train accuracy:\t\t9.75 %\n",
      "  validation accuracy:\t\t9.74 %\n"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, that's even worse, we get the gradient explosion with ELU and can't train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.25) residual connection (connection may bypass 1 conv layer, or you may stack 2 pairs of convs like in the original paper)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base residual block definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resBlock(incoming, num_filters):\n",
    "    conv1 = L.layers.Conv2DLayer(incoming, num_filters=num_filters, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01), pad='same')\n",
    "    conv2 = L.layers.Conv2DLayer(conv1, num_filters=num_filters, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01), pad='same')\n",
    "    shortcut = L.layers.Conv2DLayer(incoming, num_filters=num_filters, filter_size=1, nonlinearity=None, b=None)\n",
    "    add = L.layers.ElemwiseSumLayer([conv2, shortcut])\n",
    "    \n",
    "    return add   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we replace convolutional layer pairs with resBlocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "res1        = resBlock(input_layer, 8)\n",
    "pool1       = L.layers.MaxPool2DLayer(res1, pool_size=(2, 2))\n",
    "res2        = resBlock(input_layer, 12)\n",
    "pool2       = L.layers.MaxPool2DLayer(res2, pool_size=(2, 2))\n",
    "conv3       = L.layers.Conv2DLayer(pool2, num_filters=16, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "pool3       = L.layers.GlobalPoolLayer(conv3, pool_function=theano.tensor.max)\n",
    "dense       = L.layers.DenseLayer(pool3, num_units=10, nonlinearity=L.nonlinearities.softmax)\n",
    "\n",
    "predicted_y = L.layers.get_output(dense)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(predicted_y, target_y).mean()\n",
    "accuracy = L.objectives.categorical_accuracy(predicted_y, target_y).mean()\n",
    "updates = L.updates.adamax(loss, L.layers.get_all_params(dense, trainable=True))\n",
    "\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 95.949s\n",
      "  training loss (in-iteration):\t\t2.103647\n",
      "  train accuracy:\t\t32.63 %\n",
      "  validation accuracy:\t\t52.35 %\n",
      "Epoch 2 of 10 took 94.768s\n",
      "  training loss (in-iteration):\t\t1.212154\n",
      "  train accuracy:\t\t62.31 %\n",
      "  validation accuracy:\t\t72.93 %\n",
      "Epoch 3 of 10 took 95.628s\n",
      "  training loss (in-iteration):\t\t0.741283\n",
      "  train accuracy:\t\t76.67 %\n",
      "  validation accuracy:\t\t81.11 %\n",
      "Epoch 4 of 10 took 95.568s\n",
      "  training loss (in-iteration):\t\t0.571315\n",
      "  train accuracy:\t\t82.07 %\n",
      "  validation accuracy:\t\t83.92 %\n",
      "Epoch 5 of 10 took 94.914s\n",
      "  training loss (in-iteration):\t\t0.483713\n",
      "  train accuracy:\t\t84.94 %\n",
      "  validation accuracy:\t\t86.41 %\n",
      "Epoch 6 of 10 took 94.766s\n",
      "  training loss (in-iteration):\t\t0.424209\n",
      "  train accuracy:\t\t86.78 %\n",
      "  validation accuracy:\t\t87.97 %\n",
      "Epoch 7 of 10 took 93.931s\n",
      "  training loss (in-iteration):\t\t0.380580\n",
      "  train accuracy:\t\t88.19 %\n",
      "  validation accuracy:\t\t88.99 %\n",
      "Epoch 8 of 10 took 95.944s\n",
      "  training loss (in-iteration):\t\t0.347996\n",
      "  train accuracy:\t\t89.21 %\n",
      "  validation accuracy:\t\t90.08 %\n",
      "Epoch 9 of 10 took 94.278s\n",
      "  training loss (in-iteration):\t\t0.318464\n",
      "  train accuracy:\t\t90.12 %\n",
      "  validation accuracy:\t\t90.77 %\n",
      "Epoch 10 of 10 took 95.426s\n",
      "  training loss (in-iteration):\t\t0.298000\n",
      "  train accuracy:\t\t90.86 %\n",
      "  validation accuracy:\t\t91.61 %\n"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our mini-ResNet starts pretty fast, but after 10 epochs accuracy is lower than for baseline net. It's possible that this architecture behaves better for more complicated datasets, like CIFAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.25) conv maxout network (4 units within one maxout unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "conv1_1     = L.layers.Conv2DLayer(input_layer, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "conv1_2     = L.layers.Conv2DLayer(conv1_1, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "maxout1     = L.layers.FeaturePoolLayer(conv1_2, pool_size=4)\n",
    "pool1       = L.layers.MaxPool2DLayer(maxout1, pool_size=(2, 2))\n",
    "conv2_1     = L.layers.Conv2DLayer(pool1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "conv2_2     = L.layers.Conv2DLayer(conv2_1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "maxout2     = L.layers.FeaturePoolLayer(conv2_2, pool_size=4)\n",
    "pool2       = L.layers.MaxPool2DLayer(maxout2, pool_size=(2, 2))\n",
    "conv3       = L.layers.Conv2DLayer(pool2, num_filters=16, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "pool3       = L.layers.MaxPool2DLayer(conv3, pool_size=(2, 2))\n",
    "dense       = L.layers.DenseLayer(pool3, num_units=10, nonlinearity=L.nonlinearities.softmax)\n",
    "\n",
    "predicted_y = L.layers.get_output(dense)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(predicted_y, target_y).mean()\n",
    "accuracy = L.objectives.categorical_accuracy(predicted_y, target_y).mean()\n",
    "updates = L.updates.adamax(loss, L.layers.get_all_params(dense, trainable=True))\n",
    "\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 49.743s\n",
      "  training loss (in-iteration):\t\t1.797599\n",
      "  train accuracy:\t\t36.10 %\n",
      "  validation accuracy:\t\t69.49 %\n",
      "Epoch 2 of 10 took 49.429s\n",
      "  training loss (in-iteration):\t\t0.741970\n",
      "  train accuracy:\t\t76.88 %\n",
      "  validation accuracy:\t\t82.34 %\n",
      "Epoch 3 of 10 took 48.987s\n",
      "  training loss (in-iteration):\t\t0.516449\n",
      "  train accuracy:\t\t84.12 %\n",
      "  validation accuracy:\t\t86.41 %\n",
      "Epoch 4 of 10 took 47.667s\n",
      "  training loss (in-iteration):\t\t0.419172\n",
      "  train accuracy:\t\t87.11 %\n",
      "  validation accuracy:\t\t89.25 %\n",
      "Epoch 5 of 10 took 49.223s\n",
      "  training loss (in-iteration):\t\t0.353978\n",
      "  train accuracy:\t\t89.12 %\n",
      "  validation accuracy:\t\t90.81 %\n",
      "Epoch 6 of 10 took 49.512s\n",
      "  training loss (in-iteration):\t\t0.309210\n",
      "  train accuracy:\t\t90.65 %\n",
      "  validation accuracy:\t\t91.60 %\n",
      "Epoch 7 of 10 took 49.156s\n",
      "  training loss (in-iteration):\t\t0.278759\n",
      "  train accuracy:\t\t91.57 %\n",
      "  validation accuracy:\t\t92.71 %\n",
      "Epoch 8 of 10 took 48.884s\n",
      "  training loss (in-iteration):\t\t0.254703\n",
      "  train accuracy:\t\t92.38 %\n",
      "  validation accuracy:\t\t93.30 %\n",
      "Epoch 9 of 10 took 48.418s\n",
      "  training loss (in-iteration):\t\t0.234530\n",
      "  train accuracy:\t\t92.95 %\n",
      "  validation accuracy:\t\t93.88 %\n",
      "Epoch 10 of 10 took 49.823s\n",
      "  training loss (in-iteration):\t\t0.222483\n",
      "  train accuracy:\t\t93.27 %\n",
      "  validation accuracy:\t\t93.75 %\n"
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to the previous case: fast start, but lower accuracy after 10 epochs. But we can also notice that training time decreased, since each maxout layer halves the amount of channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.5) replace convolution with fire module from SqueezeNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a fire module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fireBlock(incoming, num_s1, num_e1, num_e3):\n",
    "    conv_s  = L.layers.Conv2DLayer(incoming, num_filters=num_s1, filter_size=1, nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01), pad='same')\n",
    "    conv_e1 = L.layers.Conv2DLayer(conv_s, num_filters=num_e1, filter_size=1, nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01), pad='same')\n",
    "    conv_e3 = L.layers.Conv2DLayer(conv_s, num_filters=num_e3, filter_size=3, nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01), pad='same')\n",
    "    concat  = L.layers.ConcatLayer([conv_e1, conv_e3], axis=1)\n",
    "    \n",
    "    return concat   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline architecture with all convolutions replaced by fire modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_X)\n",
    "fire1_1     = fireBlock(input_layer, 8, 8, 8)\n",
    "fire1_2     = fireBlock(fire1_1, 8, 8, 8)\n",
    "pool1       = L.layers.MaxPool2DLayer(fire1_2, pool_size=(2, 2))\n",
    "fire2_1     = fireBlock(pool1, 12, 12, 12)\n",
    "fire2_2     = fireBlock(fire2_1, 12, 12, 12)\n",
    "pool2       = L.layers.MaxPool2DLayer(fire2_2, pool_size=(2, 2))\n",
    "fire3       = fireBlock(pool2, 16, 16, 16)\n",
    "pool3       = L.layers.MaxPool2DLayer(fire3, pool_size=(2, 2))\n",
    "dense       = L.layers.DenseLayer(pool3, num_units=10, nonlinearity=L.nonlinearities.softmax)\n",
    "\n",
    "predicted_y = L.layers.get_output(dense)\n",
    "\n",
    "loss = L.objectives.categorical_crossentropy(predicted_y, target_y).mean()\n",
    "accuracy = L.objectives.categorical_accuracy(predicted_y, target_y).mean()\n",
    "updates = L.updates.adamax(loss, L.layers.get_all_params(dense, trainable=True))\n",
    "\n",
    "train_fun = theano.function([input_X, target_y], [loss, accuracy], updates=updates)\n",
    "accuracy_fun = theano.function([input_X, target_y], accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10 took 216.430s\n",
      "  training loss (in-iteration):\t\t1.096469\n",
      "  train accuracy:\t\t65.08 %\n",
      "  validation accuracy:\t\t92.66 %\n",
      "Epoch 2 of 10 took 216.319s\n",
      "  training loss (in-iteration):\t\t0.203112\n",
      "  train accuracy:\t\t93.74 %\n",
      "  validation accuracy:\t\t95.55 %\n",
      "Epoch 3 of 10 took 216.477s\n",
      "  training loss (in-iteration):\t\t0.137267\n",
      "  train accuracy:\t\t95.75 %\n",
      "  validation accuracy:\t\t96.32 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-c2bffa4d5f17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-a85ea1e6d9e3>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(num_epochs, batch_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_err_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_acc_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stopped training after just 3 epochs, since I'm short on time, but fire blocks seem to work very well (validation accuracy >90% after first epoch, beats the baseline after 3 epochs). They also take more time to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.25) train GAN on MNIST;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I also tried this, but didn't succeed. Maybe I just needed more iterations or different hyperparameters. Some intermediary results are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code_size = 100\n",
    "noise = T.matrix('noise')\n",
    "\n",
    "gen_input_layer = L.layers.InputLayer([None, code_size], input_var=noise)\n",
    "\n",
    "gen_dense = L.layers.DenseLayer(gen_input_layer, 128 * 7 * 7, nonlinearity=T.nnet.elu)\n",
    "\n",
    "gen_reshape = L.layers.ReshapeLayer(gen_dense, (-1, 128, 7, 7))\n",
    "gen_deconv1 = L.layers.Deconv2DLayer(gen_reshape, 64, filter_size=5, stride=2, nonlinearity=T.nnet.elu)\n",
    "gen_deconv2 = L.layers.Deconv2DLayer(gen_deconv1, 32, filter_size=5, stride=2, crop=3, nonlinearity=T.nnet.elu)\n",
    "gen_output = L.layers.Conv2DLayer(gen_deconv2, 1, filter_size=4, nonlinearity=T.nnet.elu)\n",
    "#print (\"Generator output:\", gen_deconv3.output_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used maxout-modification for discriminator network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = T.tensor4('inputs')\n",
    "\n",
    "disc_input_layer = L.layers.InputLayer(shape=input_shape, input_var=input_image)\n",
    "disc_conv1_1     = L.layers.Conv2DLayer(disc_input_layer, num_filters=8, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "disc_maxout1     = L.layers.FeaturePoolLayer(disc_conv1_1, pool_size=4)\n",
    "disc_pool1       = L.layers.MaxPool2DLayer(disc_maxout1, pool_size=(2, 2))\n",
    "disc_conv2_1     = L.layers.Conv2DLayer(disc_pool1, num_filters=12, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "disc_maxout2     = L.layers.FeaturePoolLayer(disc_conv2_1, pool_size=4)\n",
    "disc_pool2       = L.layers.MaxPool2DLayer(disc_maxout2, pool_size=(2, 2))\n",
    "disc_conv3       = L.layers.Conv2DLayer(disc_pool2, num_filters=16, filter_size=(3, 3), nonlinearity=L.nonlinearities.LeakyRectify(leakiness=0.01))\n",
    "disc_pool3       = L.layers.MaxPool2DLayer(disc_conv3, pool_size=(2, 2))\n",
    "disc_output      = L.layers.DenseLayer(disc_pool3, num_units=1, nonlinearity=L.nonlinearities.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_out  = L.layers.get_output(disc_output)\n",
    "gen_out   = L.layers.get_output(disc_output, L.layers.get_output(gen_output))\n",
    "\n",
    "gen_loss  = L.objectives.binary_crossentropy(gen_out, 1).mean()\n",
    "disc_loss = (L.objectives.binary_crossentropy(real_out, 1) + L.objectives.binary_crossentropy(gen_out, 0)).mean()\n",
    "    \n",
    "gen_params  = L.layers.get_all_params(gen_output, trainable=True)\n",
    "disc_params = L.layers.get_all_params(disc_output, trainable=True)\n",
    "\n",
    "updates = L.updates.adam(generator_loss, generator_params)\n",
    "updates.update(L.updates.sgd(discriminator_loss, discriminator_params, 1.0))\n",
    "\n",
    "train_fn = theano.function([noise, input_image], \n",
    "                           #[(real_out > .5).mean(), (gen_out < .5).mean()], \n",
    "                           [gen_loss, disc_loss]\n",
    "                           updates=updates)\n",
    "    \n",
    "gen_fn = theano.function([noise], L.layers.get_output(gen_output, deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.  1.]\n",
      "  training loss:\t\t[ 0.          0.99913194]\n",
      "  training loss:\t\t[ 0.1       0.984375]\n",
      "  training loss:\t\t[ 0.18181818  0.98082386]\n",
      "  training loss:\t\t[ 0.25        0.98046875]\n",
      "  training loss:\t\t[ 0.30769231  0.98076923]\n",
      "  training loss:\t\t[ 0.35714286  0.98158482]\n",
      "  training loss:\t\t[ 0.4        0.9828125]\n",
      "  training loss:\t\t[ 0.4375      0.98388672]\n",
      "  training loss:\t\t[ 0.47058824  0.98483456]\n",
      "  training loss:\t\t[ 0.5         0.98567708]\n",
      "  training loss:\t\t[ 0.52631579  0.98643092]\n",
      "  training loss:\t\t[ 0.55        0.98710937]\n",
      "  training loss:\t\t[ 0.57142857  0.98772321]\n",
      "  training loss:\t\t[ 0.59090909  0.98828125]\n",
      "  training loss:\t\t[ 0.60869565  0.98879076]\n",
      "  training loss:\t\t[ 0.625       0.98925781]\n",
      "  training loss:\t\t[ 0.64       0.9896875]\n",
      "  training loss:\t\t[ 0.65384615  0.99008413]\n",
      "  training loss:\t\t[ 0.66666667  0.99045139]\n",
      "  training loss:\t\t[ 0.67857143  0.99079241]\n",
      "  training loss:\t\t[ 0.68965517  0.99110991]\n",
      "  training loss:\t\t[ 0.7         0.99140625]\n",
      "  training loss:\t\t[ 0.70967742  0.99168347]\n",
      "  training loss:\t\t[ 0.71875     0.99194336]\n",
      "  training loss:\t\t[ 0.72727273  0.9921875 ]\n",
      "  training loss:\t\t[ 0.73529412  0.99241728]\n",
      "  training loss:\t\t[ 0.74285714  0.99263393]\n",
      "  training loss:\t\t[ 0.75        0.99283854]\n",
      "  training loss:\t\t[ 0.75675676  0.99303209]\n",
      "  training loss:\t\t[ 0.76315789  0.99321546]\n",
      "  training loss:\t\t[ 0.76923077  0.99338942]\n",
      "  training loss:\t\t[ 0.775       0.99355469]\n",
      "  training loss:\t\t[ 0.7804878   0.99371189]\n",
      "  training loss:\t\t[ 0.78571429  0.99386161]\n",
      "  training loss:\t\t[ 0.79069767  0.99400436]\n",
      "  training loss:\t\t[ 0.79545455  0.99414062]\n",
      "  training loss:\t\t[ 0.8         0.99427083]\n",
      "  training loss:\t\t[ 0.80434783  0.99439538]\n",
      "  training loss:\t\t[ 0.80851064  0.99451463]\n",
      "  training loss:\t\t[ 0.8125      0.99462891]\n",
      "  training loss:\t\t[ 0.81632653  0.99473852]\n",
      "  training loss:\t\t[ 0.82        0.99484375]\n",
      "  training loss:\t\t[ 0.82352941  0.99494485]\n",
      "  training loss:\t\t[ 0.82692308  0.99504207]\n",
      "  training loss:\t\t[ 0.83018868  0.99513561]\n",
      "  training loss:\t\t[ 0.83333333  0.99522569]\n",
      "  training loss:\t\t[ 0.83636364  0.9953125 ]\n",
      "  training loss:\t\t[ 0.83928571  0.99539621]\n",
      "  training loss:\t\t[ 0.84210526  0.99547697]\n",
      "  training loss:\t\t[ 0.84482759  0.99555496]\n",
      "  training loss:\t\t[ 0.84745763  0.9956303 ]\n",
      "  training loss:\t\t[ 0.85        0.99570313]\n",
      "  training loss:\t\t[ 0.85245902  0.99577357]\n",
      "  training loss:\t\t[ 0.85483871  0.99584173]\n",
      "  training loss:\t\t[ 0.85714286  0.99590774]\n",
      "  training loss:\t\t[ 0.859375    0.99597168]\n",
      "  training loss:\t\t[ 0.86153846  0.99603365]\n",
      "  training loss:\t\t[ 0.86363636  0.99609375]\n",
      "  training loss:\t\t[ 0.86567164  0.99615205]\n",
      "  training loss:\t\t[ 0.86764706  0.99620864]\n",
      "  training loss:\t\t[ 0.86956522  0.99626359]\n",
      "  training loss:\t\t[ 0.87142857  0.99631696]\n",
      "  training loss:\t\t[ 0.87323944  0.99636884]\n",
      "  training loss:\t\t[ 0.875       0.99641927]\n",
      "  training loss:\t\t[ 0.87671233  0.99646832]\n",
      "  training loss:\t\t[ 0.87837838  0.99651605]\n",
      "  training loss:\t\t[ 0.88       0.9965625]\n",
      "  training loss:\t\t[ 0.88157895  0.99660773]\n",
      "  training loss:\t\t[ 0.88311688  0.99665179]\n",
      "  training loss:\t\t[ 0.88461538  0.99669471]\n",
      "  training loss:\t\t[ 0.88607595  0.99673655]\n",
      "  training loss:\t\t[ 0.8875      0.99677734]\n",
      "  training loss:\t\t[ 0.88888889  0.99681713]\n",
      "  training loss:\t\t[ 0.8902439   0.99685595]\n",
      "  training loss:\t\t[ 0.89156627  0.99689383]\n",
      "  training loss:\t\t[ 0.89285714  0.9969308 ]\n",
      "  training loss:\t\t[ 0.89411765  0.99696691]\n",
      "  training loss:\t\t[ 0.89534884  0.99700218]\n",
      "  training loss:\t\t[ 0.89655172  0.99703664]\n",
      "  training loss:\t\t[ 0.89772727  0.99707031]\n",
      "  training loss:\t\t[ 0.8988764   0.99710323]\n",
      "  training loss:\t\t[ 0.9         0.99713542]\n",
      "  training loss:\t\t[ 0.9010989  0.9971669]\n",
      "  training loss:\t\t[ 0.90217391  0.99719769]\n",
      "  training loss:\t\t[ 0.90322581  0.99722782]\n",
      "  training loss:\t\t[ 0.90425532  0.99725731]\n",
      "  training loss:\t\t[ 0.90526316  0.99728618]\n",
      "  training loss:\t\t[ 0.90625     0.99731445]\n",
      "  training loss:\t\t[ 0.90721649  0.99734214]\n",
      "  training loss:\t\t[ 0.90816327  0.99736926]\n",
      "  training loss:\t\t[ 0.90909091  0.99739583]\n",
      "  training loss:\t\t[ 0.91        0.99742188]\n",
      "  training loss:\t\t[ 0.91089109  0.9974474 ]\n",
      "  training loss:\t\t[ 0.91176471  0.99747243]\n",
      "  training loss:\t\t[ 0.91262136  0.99749697]\n",
      "  training loss:\t\t[ 0.91346154  0.99752103]\n",
      "  training loss:\t\t[ 0.91428571  0.99754464]\n",
      "  training loss:\t\t[ 0.91509434  0.99756781]\n",
      "  training loss:\t\t[ 0.91588785  0.99759054]\n",
      "  training loss:\t\t[ 0.91666667  0.99761285]\n",
      "  training loss:\t\t[ 0.91743119  0.99763475]\n",
      "  training loss:\t\t[ 0.91818182  0.99765625]\n",
      "  training loss:\t\t[ 0.91891892  0.98866836]\n",
      "  training loss:\t\t[ 0.91071429  0.98876953]\n",
      "  training loss:\t\t[ 0.90265487  0.98886892]\n",
      "  training loss:\t\t[ 0.89473684  0.98896656]\n",
      "  training loss:\t\t[ 0.89565217  0.9890625 ]\n",
      "  training loss:\t\t[ 0.89655172  0.9805361 ]\n",
      "  training loss:\t\t[ 0.8974359   0.97215545]\n",
      "  training loss:\t\t[ 0.89830508  0.96391684]\n",
      "  training loss:\t\t[ 0.89915966  0.9558167 ]\n",
      "  training loss:\t\t[ 0.89375    0.9561849]\n",
      "  training loss:\t\t[ 0.88636364  0.94828254]\n",
      "  training loss:\t\t[ 0.87909836  0.94050973]\n",
      "  training loss:\t\t[ 0.87195122  0.93286331]\n",
      "  training loss:\t\t[ 0.86491935  0.92534022]\n",
      "  training loss:\t\t[ 0.858      0.9179375]\n",
      "  training loss:\t\t[ 0.85119048  0.91065228]\n",
      "  training loss:\t\t[ 0.84448819  0.90348179]\n",
      "  training loss:\t\t[ 0.83789062  0.89642334]\n",
      "  training loss:\t\t[ 0.83139535  0.88947432]\n",
      "  training loss:\t\t[ 0.825       0.88263221]\n",
      "  training loss:\t\t[ 0.81870229  0.87589456]\n",
      "  training loss:\t\t[ 0.8125    0.869259]\n",
      "  training loss:\t\t[ 0.80639098  0.86272321]\n",
      "  training loss:\t\t[ 0.80037313  0.85628498]\n",
      "  training loss:\t\t[ 0.79444444  0.84994213]\n",
      "  training loss:\t\t[ 0.78860294  0.84369256]\n",
      "  training loss:\t\t[ 0.78284672  0.83753422]\n",
      "  training loss:\t\t[ 0.77717391  0.83146513]\n",
      "  training loss:\t\t[ 0.77158273  0.82548336]\n",
      "  training loss:\t\t[ 0.76607143  0.81958705]\n",
      "  training loss:\t\t[ 0.7606383   0.81377438]\n",
      "  training loss:\t\t[ 0.75528169  0.80804357]\n",
      "  training loss:\t\t[ 0.75        0.80239292]\n",
      "  training loss:\t\t[ 0.74479167  0.79682075]\n",
      "  training loss:\t\t[ 0.73965517  0.79132543]\n",
      "  training loss:\t\t[ 0.73458904  0.78590539]\n",
      "  training loss:\t\t[ 0.72959184  0.7805591 ]\n",
      "  training loss:\t\t[ 0.72466216  0.77528505]\n",
      "  training loss:\t\t[ 0.71979866  0.7700818 ]\n",
      "  training loss:\t\t[ 0.715       0.76494792]\n",
      "  training loss:\t\t[ 0.7102649   0.75988204]\n",
      "  training loss:\t\t[ 0.70559211  0.75488281]\n",
      "  training loss:\t\t[ 0.70098039  0.74994894]\n",
      "  training loss:\t\t[ 0.69642857  0.74507914]\n",
      "  training loss:\t\t[ 0.69193548  0.74027218]\n",
      "  training loss:\t\t[ 0.6875      0.73552684]\n",
      "  training loss:\t\t[ 0.68312102  0.73084196]\n",
      "  training loss:\t\t[ 0.67879747  0.72621638]\n",
      "  training loss:\t\t[ 0.6745283   0.72164898]\n",
      "  training loss:\t\t[ 0.6703125   0.71713867]\n",
      "  training loss:\t\t[ 0.66614907  0.71268439]\n",
      "  training loss:\t\t[ 0.66203704  0.70828511]\n",
      "  training loss:\t\t[ 0.65797546  0.7039398 ]\n",
      "  training loss:\t\t[ 0.65396341  0.69964748]\n",
      "  training loss:\t\t[ 0.65       0.6954072]\n",
      "  training loss:\t\t[ 0.64608434  0.691218  ]\n",
      "  training loss:\t\t[ 0.64221557  0.68707897]\n",
      "  training loss:\t\t[ 0.63839286  0.68298921]\n",
      "  training loss:\t\t[ 0.63461538  0.67894786]\n",
      "  training loss:\t\t[ 0.63088235  0.67495404]\n",
      "  training loss:\t\t[ 0.62719298  0.67100694]\n",
      "  training loss:\t\t[ 0.62354651  0.66710574]\n",
      "  training loss:\t\t[ 0.6199422   0.66324964]\n",
      "  training loss:\t\t[ 0.61637931  0.65943786]\n",
      "  training loss:\t\t[ 0.61285714  0.65566964]\n",
      "  training loss:\t\t[ 0.609375    0.65194425]\n",
      "  training loss:\t\t[ 0.6059322   0.64826095]\n",
      "  training loss:\t\t[ 0.60252809  0.64461903]\n",
      "  training loss:\t\t[ 0.59916201  0.64101781]\n",
      "  training loss:\t\t[ 0.59583333  0.6374566 ]\n",
      "  training loss:\t\t[ 0.59254144  0.63393474]\n",
      "  training loss:\t\t[ 0.58928571  0.63045158]\n",
      "  training loss:\t\t[ 0.58606557  0.62700649]\n",
      "  training loss:\t\t[ 0.58288043  0.62359885]\n",
      "  training loss:\t\t[ 0.57972973  0.62022804]\n",
      "  training loss:\t\t[ 0.5766129   0.61689348]\n",
      "  training loss:\t\t[ 0.57352941  0.61359459]\n",
      "  training loss:\t\t[ 0.57047872  0.61033078]\n",
      "  training loss:\t\t[ 0.56746032  0.60710152]\n",
      "  training loss:\t\t[ 0.56447368  0.60390625]\n",
      "  training loss:\t\t[ 0.56151832  0.60074444]\n",
      "  training loss:\t\t[ 0.55859375  0.59761556]\n",
      "  training loss:\t\t[ 0.55569948  0.59451911]\n",
      "  training loss:\t\t[ 0.55283505  0.59145457]\n",
      "  training loss:\t\t[ 0.55        0.58842147]\n",
      "  training loss:\t\t[ 0.54719388  0.58541932]\n",
      "  training loss:\t\t[ 0.54441624  0.58244765]\n",
      "  training loss:\t\t[ 0.54166667  0.579506  ]\n",
      "  training loss:\t\t[ 0.53894472  0.57659391]\n",
      "  training loss:\t\t[ 0.53625     0.57371094]\n",
      "  training loss:\t\t[ 0.53358209  0.57085665]\n",
      "  training loss:\t\t[ 0.53094059  0.56803063]\n",
      "  training loss:\t\t[ 0.52832512  0.56523245]\n",
      "  training loss:\t\t[ 0.52573529  0.5624617 ]\n",
      "  training loss:\t\t[ 0.52317073  0.55971799]\n",
      "  training loss:\t\t[ 0.52063107  0.55700091]\n",
      "  training loss:\t\t[ 0.51811594  0.55431008]\n",
      "  training loss:\t\t[ 0.515625    0.55164513]\n",
      "  training loss:\t\t[ 0.51315789  0.54900568]\n",
      "  training loss:\t\t[ 0.51071429  0.54639137]\n",
      "  training loss:\t\t[ 0.50829384  0.54380184]\n",
      "  training loss:\t\t[ 0.50589623  0.54123673]\n",
      "  training loss:\t\t[ 0.50352113  0.53869572]\n",
      "  training loss:\t\t[ 0.50116822  0.53617845]\n",
      "  training loss:\t\t[ 0.49883721  0.53368459]\n",
      "  training loss:\t\t[ 0.49652778  0.53121383]\n",
      "  training loss:\t\t[ 0.49423963  0.52876584]\n",
      "  training loss:\t\t[ 0.49197248  0.52634031]\n",
      "  training loss:\t\t[ 0.48972603  0.52393693]\n",
      "  training loss:\t\t[ 0.4875     0.5215554]\n",
      "  training loss:\t\t[ 0.48529412  0.51919542]\n",
      "  training loss:\t\t[ 0.48310811  0.5168567 ]\n",
      "  training loss:\t\t[ 0.4809417   0.51453896]\n",
      "  training loss:\t\t[ 0.47879464  0.51224191]\n",
      "  training loss:\t\t[ 0.47666667  0.50996528]\n",
      "  training loss:\t\t[ 0.47455752  0.50770879]\n",
      "  training loss:\t\t[ 0.47246696  0.50547219]\n",
      "  training loss:\t\t[ 0.47039474  0.50325521]\n",
      "  training loss:\t\t[ 0.46834061  0.50105759]\n",
      "  training loss:\t\t[ 0.46630435  0.49887908]\n",
      "  training loss:\t\t[ 0.46428571  0.49671943]\n",
      "  training loss:\t\t[ 0.46228448  0.49457839]\n",
      "  training loss:\t\t[ 0.46030043  0.49245574]\n",
      "  training loss:\t\t[ 0.45833333  0.49035123]\n",
      "  training loss:\t\t[ 0.45638298  0.48826463]\n",
      "  training loss:\t\t[ 0.45444915  0.48619571]\n",
      "  training loss:\t\t[ 0.45253165  0.48414425]\n",
      "  training loss:\t\t[ 0.45063025  0.48211003]\n",
      "  training loss:\t\t[ 0.44874477  0.48009283]\n",
      "  training loss:\t\t[ 0.446875    0.47809245]\n",
      "  training loss:\t\t[ 0.44502075  0.47610866]\n",
      "  training loss:\t\t[ 0.44318182  0.47414127]\n",
      "  training loss:\t\t[ 0.44135802  0.47219007]\n",
      "  training loss:\t\t[ 0.43954918  0.47025487]\n",
      "  training loss:\t\t[ 0.4377551   0.46833546]\n",
      "  training loss:\t\t[ 0.43597561  0.46643166]\n",
      "  training loss:\t\t[ 0.43421053  0.46454327]\n",
      "  training loss:\t\t[ 0.43245968  0.46267011]\n",
      "  training loss:\t\t[ 0.43072289  0.460812  ]\n",
      "  training loss:\t\t[ 0.429       0.45896875]\n",
      "  training loss:\t\t[ 0.42729084  0.45714019]\n",
      "  training loss:\t\t[ 0.42559524  0.45532614]\n",
      "  training loss:\t\t[ 0.42391304  0.45352643]\n",
      "  training loss:\t\t[ 0.42224409  0.4517409 ]\n",
      "  training loss:\t\t[ 0.42058824  0.44996936]\n",
      "  training loss:\t\t[ 0.41894531  0.44821167]\n",
      "  training loss:\t\t[ 0.41731518  0.44646766]\n",
      "  training loss:\t\t[ 0.41569767  0.44473716]\n",
      "  training loss:\t\t[ 0.41409266  0.44302003]\n",
      "  training loss:\t\t[ 0.4125      0.44131611]\n",
      "  training loss:\t\t[ 0.41091954  0.43962524]\n",
      "  training loss:\t\t[ 0.40935115  0.43794728]\n",
      "  training loss:\t\t[ 0.40779468  0.43628208]\n",
      "  training loss:\t\t[ 0.40625    0.4346295]\n",
      "  training loss:\t\t[ 0.40471698  0.43298939]\n",
      "  training loss:\t\t[ 0.40319549  0.43136161]\n",
      "  training loss:\t\t[ 0.40168539  0.42974602]\n",
      "  training loss:\t\t[ 0.40018657  0.42814249]\n",
      "  training loss:\t\t[ 0.39869888  0.42655088]\n",
      "  training loss:\t\t[ 0.39722222  0.42497106]\n",
      "  training loss:\t\t[ 0.39575646  0.42340291]\n",
      "  training loss:\t\t[ 0.39430147  0.42184628]\n",
      "  training loss:\t\t[ 0.39285714  0.42030105]\n",
      "  training loss:\t\t[ 0.39142336  0.41876711]\n",
      "  training loss:\t\t[ 0.39        0.41724432]\n",
      "  training loss:\t\t[ 0.38858696  0.41573256]\n",
      "  training loss:\t\t[ 0.38718412  0.41423172]\n",
      "  training loss:\t\t[ 0.38579137  0.41274168]\n",
      "  training loss:\t\t[ 0.3844086   0.41126232]\n",
      "  training loss:\t\t[ 0.38303571  0.40979353]\n",
      "  training loss:\t\t[ 0.3816726   0.40833519]\n",
      "  training loss:\t\t[ 0.38031915  0.40688719]\n",
      "  training loss:\t\t[ 0.37897527  0.40544943]\n",
      "  training loss:\t\t[ 0.37764085  0.40402179]\n",
      "  training loss:\t\t[ 0.37631579  0.40260417]\n",
      "  training loss:\t\t[ 0.375       0.40119646]\n",
      "  training loss:\t\t[ 0.37369338  0.39979856]\n",
      "  training loss:\t\t[ 0.37239583  0.39841037]\n",
      "  training loss:\t\t[ 0.37110727  0.39703179]\n",
      "  training loss:\t\t[ 0.36982759  0.39566272]\n",
      "  training loss:\t\t[ 0.3685567   0.39430305]\n",
      "  training loss:\t\t[ 0.36729452  0.3929527 ]\n",
      "  training loss:\t\t[ 0.36604096  0.39161156]\n",
      "  training loss:\t\t[ 0.36479592  0.39027955]\n",
      "  training loss:\t\t[ 0.36355932  0.38895657]\n",
      "  training loss:\t\t[ 0.36233108  0.38764253]\n",
      "  training loss:\t\t[ 0.36111111  0.38633733]\n",
      "  training loss:\t\t[ 0.35989933  0.3850409 ]\n",
      "  training loss:\t\t[ 0.35869565  0.38375314]\n",
      "  training loss:\t\t[ 0.3575      0.38247396]\n",
      "  training loss:\t\t[ 0.35631229  0.38120328]\n",
      "  training loss:\t\t[ 0.35513245  0.37994102]\n",
      "  training loss:\t\t[ 0.3539604   0.37868709]\n",
      "  training loss:\t\t[ 0.35279605  0.37744141]\n",
      "  training loss:\t\t[ 0.35163934  0.37620389]\n",
      "  training loss:\t\t[ 0.3504902   0.37497447]\n",
      "  training loss:\t\t[ 0.34934853  0.37375305]\n",
      "  training loss:\t\t[ 0.34821429  0.37253957]\n",
      "  training loss:\t\t[ 0.34708738  0.37133394]\n",
      "  training loss:\t\t[ 0.34596774  0.37013609]\n",
      "  training loss:\t\t[ 0.34485531  0.36894594]\n",
      "  training loss:\t\t[ 0.34375     0.36776342]\n",
      "  training loss:\t\t[ 0.34265176  0.36658846]\n",
      "  training loss:\t\t[ 0.34156051  0.36542098]\n",
      "  training loss:\t\t[ 0.34047619  0.36426091]\n",
      "  training loss:\t\t[ 0.33939873  0.36310819]\n",
      "  training loss:\t\t[ 0.33832808  0.36196274]\n",
      "  training loss:\t\t[ 0.33726415  0.36082449]\n",
      "  training loss:\t\t[ 0.3362069   0.35969338]\n",
      "  training loss:\t\t[ 0.33515625  0.35856934]\n",
      "  training loss:\t\t[ 0.33411215  0.3574523 ]\n",
      "  training loss:\t\t[ 0.33307453  0.3563422 ]\n",
      "  training loss:\t\t[ 0.33204334  0.35523897]\n",
      "  training loss:\t\t[ 0.33101852  0.35414255]\n",
      "  training loss:\t\t[ 0.33        0.35305288]\n",
      "  training loss:\t\t[ 0.32898773  0.3519699 ]\n",
      "  training loss:\t\t[ 0.32798165  0.35089354]\n",
      "  training loss:\t\t[ 0.32698171  0.34982374]\n",
      "  training loss:\t\t[ 0.32598784  0.34876045]\n",
      "  training loss:\t\t[ 0.325      0.3477036]\n",
      "  training loss:\t\t[ 0.32401813  0.34665313]\n",
      "  training loss:\t\t[ 0.32304217  0.345609  ]\n",
      "  training loss:\t\t[ 0.32207207  0.34457113]\n",
      "  training loss:\t\t[ 0.32110778  0.34353948]\n",
      "  training loss:\t\t[ 0.32014925  0.34251399]\n",
      "  training loss:\t\t[ 0.31919643  0.34149461]\n",
      "  training loss:\t\t[ 0.31824926  0.34048127]\n",
      "  training loss:\t\t[ 0.31730769  0.33947393]\n",
      "  training loss:\t\t[ 0.31637168  0.33847253]\n",
      "  training loss:\t\t[ 0.31544118  0.33747702]\n",
      "  training loss:\t\t[ 0.31451613  0.33648735]\n",
      "  training loss:\t\t[ 0.31359649  0.33550347]\n",
      "  training loss:\t\t[ 0.31268222  0.33452533]\n",
      "  training loss:\t\t[ 0.31177326  0.33355287]\n",
      "  training loss:\t\t[ 0.31086957  0.33258605]\n",
      "  training loss:\t\t[ 0.3099711   0.33162482]\n",
      "  training loss:\t\t[ 0.30907781  0.33066913]\n",
      "  training loss:\t\t[ 0.30818966  0.32971893]\n",
      "  training loss:\t\t[ 0.30730659  0.32877418]\n",
      "  training loss:\t\t[ 0.30642857  0.32783482]\n",
      "  training loss:\t\t[ 0.30555556  0.32690082]\n",
      "  training loss:\t\t[ 0.3046875   0.32597212]\n",
      "  training loss:\t\t[ 0.30382436  0.32504869]\n",
      "  training loss:\t\t[ 0.3029661   0.32413047]\n",
      "  training loss:\t\t[ 0.30211268  0.32321743]\n",
      "  training loss:\t\t[ 0.30126404  0.32230952]\n",
      "  training loss:\t\t[ 0.30042017  0.32140669]\n",
      "  training loss:\t\t[ 0.29958101  0.3205089 ]\n",
      "  training loss:\t\t[ 0.29874652  0.31961612]\n",
      "  training loss:\t\t[ 0.29791667  0.3187283 ]\n",
      "  training loss:\t\t[ 0.29709141  0.31784539]\n",
      "  training loss:\t\t[ 0.29627072  0.31696737]\n",
      "  training loss:\t\t[ 0.29545455  0.31609418]\n",
      "  training loss:\t\t[ 0.29464286  0.31522579]\n",
      "  training loss:\t\t[ 0.29383562  0.31436216]\n",
      "  training loss:\t\t[ 0.29303279  0.31350324]\n",
      "  training loss:\t\t[ 0.29223433  0.31264901]\n",
      "  training loss:\t\t[ 0.29144022  0.31179942]\n",
      "  training loss:\t\t[ 0.29065041  0.31095444]\n",
      "  training loss:\t\t[ 0.28986486  0.31011402]\n",
      "  training loss:\t\t[ 0.28908356  0.30927813]\n",
      "  training loss:\t\t[ 0.28830645  0.30844674]\n",
      "  training loss:\t\t[ 0.28753351  0.30761981]\n",
      "  training loss:\t\t[ 0.28676471  0.30679729]\n",
      "  training loss:\t\t[ 0.286       0.30597917]\n",
      "  training loss:\t\t[ 0.28523936  0.30516539]\n",
      "  training loss:\t\t[ 0.28448276  0.30435594]\n",
      "  training loss:\t\t[ 0.28373016  0.30355076]\n",
      "  training loss:\t\t[ 0.28298153  0.30274984]\n",
      "  training loss:\t\t[ 0.28223684  0.30195312]\n",
      "  training loss:\t\t[ 0.28149606  0.3011606 ]\n",
      "  training loss:\t\t[ 0.28075916  0.30037222]\n",
      "  training loss:\t\t[ 0.28002611  0.29958796]\n",
      "  training loss:\t\t[ 0.27929688  0.29880778]\n",
      "  training loss:\t\t[ 0.27857143  0.29803166]\n",
      "  training loss:\t\t[ 0.27784974  0.29725955]\n",
      "  training loss:\t\t[ 0.27713178  0.29649144]\n",
      "  training loss:\t\t[ 0.27641753  0.29572729]\n",
      "  training loss:\t\t[ 0.27570694  0.29496706]\n",
      "  training loss:\t\t[ 0.275       0.29421074]\n",
      "  training loss:\t\t[ 0.27429668  0.29345828]\n",
      "  training loss:\t\t[ 0.27359694  0.29270966]\n",
      "  training loss:\t\t[ 0.27290076  0.29196485]\n",
      "  training loss:\t\t[ 0.27220812  0.29122383]\n",
      "  training loss:\t\t[ 0.27151899  0.29048655]\n",
      "  training loss:\t\t[ 0.27083333  0.289753  ]\n",
      "  training loss:\t\t[ 0.27015113  0.28902314]\n",
      "  training loss:\t\t[ 0.26947236  0.28829695]\n",
      "  training loss:\t\t[ 0.26879699  0.2875744 ]\n",
      "  training loss:\t\t[ 0.268125    0.28685547]\n",
      "  training loss:\t\t[ 0.26745636  0.28614012]\n",
      "  training loss:\t\t[ 0.26679104  0.28542833]\n",
      "  training loss:\t\t[ 0.26612903  0.28472007]\n",
      "  training loss:\t\t[ 0.2654703   0.28401532]\n",
      "  training loss:\t\t[ 0.26481481  0.28331404]\n",
      "  training loss:\t\t[ 0.26416256  0.28261623]\n",
      "  training loss:\t\t[ 0.26351351  0.28192184]\n",
      "  training loss:\t\t[ 0.26286765  0.28123085]\n",
      "  training loss:\t\t[ 0.26222494  0.28054325]\n",
      "  training loss:\t\t[ 0.26158537  0.27985899]\n",
      "  training loss:\t\t[ 0.26094891  0.27917807]\n",
      "  training loss:\t\t[ 0.26031553  0.27850046]\n",
      "  training loss:\t\t[ 0.25968523  0.27782612]\n",
      "  training loss:\t\t[ 0.25905797  0.27715504]\n",
      "  training loss:\t\t[ 0.25843373  0.2764872 ]\n",
      "  training loss:\t\t[ 0.2578125   0.27582257]\n",
      "  training loss:\t\t[ 0.25719424  0.27516112]\n",
      "  training loss:\t\t[ 0.25657895  0.27450284]\n",
      "  training loss:\t\t[ 0.25596659  0.2738477 ]\n",
      "  training loss:\t\t[ 0.25535714  0.27319568]\n",
      "  training loss:\t\t[ 0.25475059  0.27254676]\n",
      "  training loss:\t\t[ 0.25414692  0.27190092]\n",
      "  training loss:\t\t[ 0.2535461   0.27125813]\n",
      "  training loss:\t\t[ 0.25294811  0.27061837]\n",
      "  training loss:\t\t[ 0.25235294  0.26998162]\n",
      "  training loss:\t\t[ 0.25176056  0.26934786]\n",
      "  training loss:\t\t[ 0.25117096  0.26871707]\n",
      "  training loss:\t\t[ 0.25058411  0.26808922]\n",
      "  training loss:\t\t[ 0.25        0.26746431]\n",
      "  training loss:\t\t[ 0.2494186  0.2668423]\n",
      "  training loss:\t\t[ 0.24883991  0.26622317]\n",
      "  training loss:\t\t[ 0.24826389  0.26560692]\n",
      "  training loss:\t\t[ 0.24769053  0.2649935 ]\n",
      "  training loss:\t\t[ 0.24711982  0.26438292]\n",
      "  training loss:\t\t[ 0.24655172  0.26377514]\n",
      "  training loss:\t\t[ 0.24598624  0.26317015]\n",
      "  training loss:\t\t[ 0.24542334  0.26256793]\n",
      "  training loss:\t\t[ 0.24486301  0.26196846]\n",
      "  training loss:\t\t[ 0.24430524  0.26137173]\n",
      "  training loss:\t\t[ 0.24375    0.2607777]\n",
      "  training loss:\t\t[ 0.24319728  0.26018637]\n",
      "  training loss:\t\t[ 0.24264706  0.25959771]\n",
      "  training loss:\t\t[ 0.24209932  0.25901171]\n",
      "  training loss:\t\t[ 0.24155405  0.25842835]\n",
      "  training loss:\t\t[ 0.24101124  0.25784761]\n",
      "  training loss:\t\t[ 0.24047085  0.25726948]\n",
      "  training loss:\t\t[ 0.23993289  0.25669393]\n",
      "  training loss:\t\t[ 0.23939732  0.25612095]\n",
      "  training loss:\t\t[ 0.23886414  0.25555053]\n",
      "  training loss:\t\t[ 0.23833333  0.25498264]\n",
      "  training loss:\t\t[ 0.23780488  0.25441727]\n",
      "  training loss:\t\t[ 0.23727876  0.2538544 ]\n",
      "  training loss:\t\t[ 0.23675497  0.25329401]\n",
      "  training loss:\t\t[ 0.23623348  0.2527361 ]\n",
      "  training loss:\t\t[ 0.23571429  0.25218063]\n",
      "  training loss:\t\t[ 0.23519737  0.2516276 ]\n",
      "  training loss:\t\t[ 0.23468271  0.251077  ]\n",
      "  training loss:\t\t[ 0.23417031  0.25052879]\n",
      "  training loss:\t\t[ 0.23366013  0.24998298]\n",
      "  training loss:\t\t[ 0.23315217  0.24943954]\n",
      "  training loss:\t\t[ 0.23264642  0.24889845]\n",
      "  training loss:\t\t[ 0.23214286  0.24835971]\n",
      "  training loss:\t\t[ 0.23164147  0.2478233 ]\n",
      "  training loss:\t\t[ 0.23114224  0.2472892 ]\n",
      "  training loss:\t\t[ 0.23064516  0.24675739]\n",
      "  training loss:\t\t[ 0.23015021  0.24622787]\n",
      "  training loss:\t\t[ 0.22965739  0.24570062]\n",
      "  training loss:\t\t[ 0.22916667  0.24517561]\n",
      "Epoch 1 of 100 took 2871.914s\n",
      "  training loss:\t\t[ 0.22916667  0.24517561]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n",
      "  training loss:\t\t[ 0.  0.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-03678123aab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mdldml/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mdldml/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, batch_size):\n",
    "            inputs, targets = batch\n",
    "            noise = L.utils.floatX(np.random.rand(len(inputs), code_size))\n",
    "            train_err += np.array(train_fn(noise, inputs))\n",
    "            train_batches += 1\n",
    "            \n",
    "            if train_batches % 10 == 0:\n",
    "                samples = gen_fn(L.utils.floatX(np.random.rand(42, code_size)))\n",
    "                try:\n",
    "                    import matplotlib.pyplot as plt\n",
    "                except ImportError:\n",
    "                    pass\n",
    "                else:\n",
    "                    plt.imsave('mnist_samples.png',\n",
    "                               (samples.reshape(6, 7, 28, 28)\n",
    "                                       .transpose(0, 2, 1, 3)\n",
    "                                       .reshape(6*28, 7*28)),\n",
    "                               cmap='gray')\n",
    "                \n",
    "            print(\"  training loss:\\t\\t{}\".format(train_err / train_batches))\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{}\".format(train_err / train_batches))\n",
    "\n",
    "        # And finally, we plot some generated data\n",
    "\n",
    "        # After half the epochs, we start decaying the learn rate towards zero\n",
    "        #if epoch >= num_epochs // 2:\n",
    "            #progress = float(epoch) / num_epochs\n",
    "            #eta.set_value(lasagne.utils.floatX(initial_eta*2*(1 - progress)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current result of generator is below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"mnist_samples.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
