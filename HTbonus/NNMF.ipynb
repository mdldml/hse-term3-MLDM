{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### (1.0*) implement Neural Network Matrix Factorization (without Spark):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne as L\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Function for loading dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    users, movies, ratings = [], [], []\n",
    "    \n",
    "    for line in open('ml-latest-small/ratings.csv').readlines()[1:]:\n",
    "        user, movie, rating, timestamp = line.split(',')\n",
    "        users.append(int(user))\n",
    "        movies.append(int(movie))\n",
    "        ratings.append(float(rating))\n",
    "        \n",
    "    return np.array(users), np.array(movies), np.array(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "users, movies, ratings = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Re-enumerate users and movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "users = users - 1\n",
    "\n",
    "count = 0\n",
    "movieNumbers = {}\n",
    "for movie in set(movies):\n",
    "    movieNumbers[movie] = count\n",
    "    count += 1\n",
    "    \n",
    "movies = np.array([movieNumbers[movie] for movie in movies])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Split into train and test parts (put one movie rating into test part for each user):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(users, movies, ratings):\n",
    "    indices = np.arange(len(ratings))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    users_train, movies_train, ratings_train = [], [], []\n",
    "    users_test, movies_test, ratings_test = [], [], []\n",
    "    users_set = set()\n",
    "    \n",
    "    for user, movie, rating in zip(users[indices], movies[indices], ratings[indices]):\n",
    "        if user in users_set:\n",
    "            users_train.append(user)\n",
    "            movies_train.append(movie)\n",
    "            ratings_train.append(rating)\n",
    "        else:\n",
    "            users_set.add(user)\n",
    "            users_test.append(user)\n",
    "            movies_test.append(movie)\n",
    "            ratings_test.append(rating)\n",
    "        \n",
    "    return (np.array(users_train), np.array(movies_train), np.array(ratings_train)), (np.array(users_test), np.array(movies_test), np.array(ratings_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(users, movies, ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Model parameters, input and shared variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D  = 10\n",
    "Dp = 60\n",
    "K  = 1\n",
    "\n",
    "n    = 1 + max(users)\n",
    "m    = 1 + max(movies)\n",
    "size = len(ratings)\n",
    "\n",
    "U  = theano.shared(np.zeros((n, D)))\n",
    "Up = theano.shared(np.zeros((n, Dp, K)))\n",
    "V  = theano.shared(np.zeros((m, D)))\n",
    "Vp = theano.shared(np.zeros((m, Dp, K)))\n",
    "\n",
    "U_indices = T.vector(\"users\", dtype=\"int32\")\n",
    "V_indices = T.vector(\"movies\", dtype=\"int32\")\n",
    "X         = T.vector(\"ratings\", dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Make set of features for each batch element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Ux  = U[U_indices]\n",
    "Vx  = V[V_indices]\n",
    "Upx = Up[U_indices]\n",
    "Vpx = Vp[V_indices]\n",
    "\n",
    "UVp_prods = (Upx * Vpx).sum(axis=2)  # (size, Dp, K), (size, Dp, K) -> (size, Dp)\n",
    "\n",
    "features = T.concatenate([Ux, Vx, UVp_prods], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "units = 50\n",
    "lam  = 0.0 # 1e-4\n",
    "\n",
    "input_layer = L.layers.InputLayer(shape=[None, 2 * D + Dp], input_var=features)\n",
    "hidden1     = L.layers.DenseLayer(input_layer, num_units=units, nonlinearity=L.nonlinearities.sigmoid)\n",
    "hidden2     = L.layers.DenseLayer(hidden1, num_units=units, nonlinearity=L.nonlinearities.sigmoid)\n",
    "hidden3     = L.layers.DenseLayer(hidden2, num_units=units, nonlinearity=L.nonlinearities.sigmoid)\n",
    "output      = L.layers.DenseLayer(hidden3, num_units=1, nonlinearity=None)\n",
    "\n",
    "#preds = L.layers.get_output(output)\n",
    "preds = features.sum(axis=1)\n",
    "loss = ((X - preds) ** 2).mean() + lam * ((U ** 2).sum() + (V ** 2).sum() + (Up ** 2).sum() + (Vp ** 2).sum())\n",
    "rmse = (((X - preds) ** 2).mean()) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train/predict/etc. functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nn_params = L.layers.get_all_params(output, trainable=True)\n",
    "mf_params = [U, V, Up, Vp]\n",
    "\n",
    "#train_nn = theano.function([U_indices, V_indices, X], [loss, rmse], updates=L.updates.rmsprop(loss, nn_params, learning_rate=0.001), allow_input_downcast=True)\n",
    "train_mf = theano.function([U_indices, V_indices, X], [loss, rmse], updates=L.updates.rmsprop(loss, mf_params, learning_rate=0.1), allow_input_downcast=True)\n",
    "predict  = theano.function([U_indices, V_indices], preds, allow_input_downcast=True)\n",
    "validate = theano.function([U_indices, V_indices, X], rmse, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Helper functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(users, movies, ratings, batch_size):\n",
    "    indices = np.arange(len(ratings))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_index in range(0, len(ratings) - batch_size + 1, batch_size):\n",
    "        excerpt = indices[start_index:(start_index + batch_size)]\n",
    "        #print excerpt\n",
    "        #print users\n",
    "        yield users[excerpt], movies[excerpt], ratings[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reset_weights():    \n",
    "    for v in nn_params:\n",
    "        val = v.get_value()\n",
    "        if(len(val.shape) < 2):\n",
    "            v.set_value(L.init.Constant(0.0)(val.shape))\n",
    "        else:\n",
    "            v.set_value(L.init.GlorotUniform(gain=4 * np.sqrt(3))(val.shape))\n",
    "            \n",
    "    for v in mf_params:\n",
    "        v.set_value(np.random.normal(scale=2.0, size=v.get_value().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Main loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def run_train(train_data, test_data, num_epochs=200, batch_size=500):\n",
    "    reset_weights()\n",
    "    \n",
    "    users_train, movies_train, ratings_train = train_data\n",
    "    users_test, movies_test, ratings_test = test_data\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_err = np.array([0.0, 0.0])\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for batch in iterate_minibatches(users_train, movies_train, ratings_train, batch_size):\n",
    "            users_b, movies_b, ratings_b = batch\n",
    "            #train_err += train_nn(users_b, movies_b, ratings_b)\n",
    "            train_err += train_mf(users_b, movies_b, ratings_b)\n",
    "            train_batches += 2\n",
    "\n",
    "        val_rmse = float(validate(users_test, movies_test, ratings_test))\n",
    "        \n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "\n",
    "        print(\"  train loss (in-iteration):\\t\\t{:.6f}\".format(train_err[0] / train_batches))\n",
    "        print(\"  train RMSE (in-iteration):\\t\\t{:.6f}\".format(train_err[1] / train_batches))\n",
    "        print(\"  validation RMSE:\\t\\t\\t{:.6f}\".format(val_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 200 took 1.924s\n",
      "  train loss (in-iteration):\t\t177.690024\n",
      "  train RMSE (in-iteration):\t\t8.940956\n",
      "  validation RMSE:\t\t\t14.968235\n",
      "Epoch 2 of 200 took 1.863s\n",
      "  train loss (in-iteration):\t\t33.536570\n",
      "  train RMSE (in-iteration):\t\t4.083215\n",
      "  validation RMSE:\t\t\t11.806075\n",
      "Epoch 3 of 200 took 1.913s\n",
      "  train loss (in-iteration):\t\t25.687782\n",
      "  train RMSE (in-iteration):\t\t3.566768\n",
      "  validation RMSE:\t\t\t9.083315\n",
      "Epoch 4 of 200 took 2.052s\n",
      "  train loss (in-iteration):\t\t18.519882\n",
      "  train RMSE (in-iteration):\t\t3.031097\n",
      "  validation RMSE:\t\t\t7.273048\n",
      "Epoch 5 of 200 took 2.148s\n",
      "  train loss (in-iteration):\t\t15.785176\n",
      "  train RMSE (in-iteration):\t\t2.798715\n",
      "  validation RMSE:\t\t\t6.445881\n",
      "Epoch 6 of 200 took 1.855s\n",
      "  train loss (in-iteration):\t\t13.628073\n",
      "  train RMSE (in-iteration):\t\t2.602597\n",
      "  validation RMSE:\t\t\t5.899555\n",
      "Epoch 7 of 200 took 2.154s\n",
      "  train loss (in-iteration):\t\t12.677491\n",
      "  train RMSE (in-iteration):\t\t2.510096\n",
      "  validation RMSE:\t\t\t5.630488\n",
      "Epoch 8 of 200 took 2.165s\n",
      "  train loss (in-iteration):\t\t11.964662\n",
      "  train RMSE (in-iteration):\t\t2.437501\n",
      "  validation RMSE:\t\t\t5.531766\n",
      "Epoch 9 of 200 took 2.102s\n",
      "  train loss (in-iteration):\t\t11.340399\n",
      "  train RMSE (in-iteration):\t\t2.373080\n",
      "  validation RMSE:\t\t\t5.543602\n",
      "Epoch 10 of 200 took 2.067s\n",
      "  train loss (in-iteration):\t\t10.971896\n",
      "  train RMSE (in-iteration):\t\t2.335359\n",
      "  validation RMSE:\t\t\t5.125168\n",
      "Epoch 11 of 200 took 2.238s\n",
      "  train loss (in-iteration):\t\t10.745744\n",
      "  train RMSE (in-iteration):\t\t2.311088\n",
      "  validation RMSE:\t\t\t5.097222\n"
     ]
    }
   ],
   "source": [
    "run_train(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Results are slightly worse than in original article, but maybe that happens because of slightly different model parameters or lack of training epochs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
